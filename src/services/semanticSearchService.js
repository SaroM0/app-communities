// Import the getEmbedding and generateText functions from the OpenAI service
const { getEmbedding, generateText } = require("./openaiService");

// Import the Pinecone client for vector index operations.
const pinecone = require("../config/pineconeClient");

/**
 * Performs a semantic search on a specific channel's vectorized messages.
 * @param {string} query - The natural language query.
 * @param {string|number} channelId - The channel ID to search within.
 * @param {number} topK - Number of top results to return (default is 5).
 * @returns {Promise<Array>} - An array of matching messages with metadata.
 */
async function semanticSearch(query, channelId, topK = 5) {
  try {
    // Generate the embedding for the query.
    const queryEmbedding = await getEmbedding(query);

    // Construct the index name based on the channel ID.
    const indexName = `channel-${channelId}`;

    // Get the corresponding index object from Pinecone.
    const index = pinecone.index(indexName);

    // Query the index using the query embedding.
    const queryResponse = await index.query({
      vector: queryEmbedding,
      topK: topK,
      includeValues: false,
      includeMetadata: true,
    });

    // Return the array of matching results.
    return queryResponse.matches;
  } catch (error) {
    console.error("Error during semantic search:", error);
    throw error;
  }
}

/**
 * Uses the context from semantic search results to generate an answer via OpenAI.
 * This function builds a prompt that includes relevant message texts from the channel,
 * then asks OpenAI to provide a contextual answer.
 *
 * @param {string} query - The natural language query.
 * @param {string|number} channelId - The channel ID to search within.
 * @param {number} topK - Number of top results to use as context (default is 5).
 * @returns {Promise<string>} - The answer generated by OpenAI.
 */
async function semanticQueryWithContext(query, channelId, topK = 5) {
  try {
    // Retrieve relevant messages using semantic search.
    const matches = await semanticSearch(query, channelId, topK);

    // Build a context string from the retrieved messages.
    let contextText = "";
    if (matches && matches.length > 0) {
      contextText = matches
        .map((match) => match.metadata.message_text)
        .join("\n");
    }

    // Construct a prompt that includes the context and the user's query.
    const prompt = `Based on the following context:\n${contextText}\n\nAnswer the query: "${query}"`;

    // Generate a response from OpenAI using the provided context.
    const answer = await generateText(prompt, {
      model: "gpt-4o",
      max_tokens: 1000,
      temperature: 0.7,
    });

    // Return the generated answer.
    return answer;
  } catch (error) {
    console.error("Error during semantic query with context:", error);
    throw error;
  }
}

module.exports = {
  semanticSearch,
  semanticQueryWithContext,
};
